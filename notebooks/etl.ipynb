{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-08T01:35:16.907260349Z",
     "start_time": "2023-07-08T01:35:16.017227261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from sqlite3 import Row\n",
    "\n",
    "import boto3\n",
    "import gzip\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connecting to S3 bucket"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session(region_name='eu-central-1')\n"
     ]
    }
   ],
   "source": [
    "# Creating Boto3 Session\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_region = os.getenv('AWS_REGION')\n",
    "aws_bucket_name = os.getenv('AWS_BUCKET_NAME')\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_region\n",
    ")\n",
    "print(session)\n",
    "\n",
    "prefix = 'DE/monthly/'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T01:51:55.439600311Z",
     "start_time": "2023-07-07T01:51:54.947526332Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Outputting the list of files in the bucket"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'REJFE7F2K553GJ4T', 'HostId': 'VoIj3aplrDsKwSqDfnuXJCyeK4jlCoWUtrZKIgO6Yi2BptcBfuyt1H/RwPZ+rN4/eRGlzQjOk/8=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'VoIj3aplrDsKwSqDfnuXJCyeK4jlCoWUtrZKIgO6Yi2BptcBfuyt1H/RwPZ+rN4/eRGlzQjOk/8=', 'x-amz-request-id': 'REJFE7F2K553GJ4T', 'date': 'Fri, 07 Jul 2023 01:51:57 GMT', 'x-amz-bucket-region': 'eu-central-1', 'content-type': 'application/xml', 'transfer-encoding': 'chunked', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'IsTruncated': False, 'Name': 'jobfeed-data-feeds', 'Prefix': 'DE/monthly/', 'Delimiter': '/', 'MaxKeys': 1000, 'CommonPrefixes': [{'Prefix': 'DE/monthly/2020-06/'}, {'Prefix': 'DE/monthly/2020-07/'}, {'Prefix': 'DE/monthly/2020-08/'}, {'Prefix': 'DE/monthly/2020-09/'}, {'Prefix': 'DE/monthly/2020-10/'}, {'Prefix': 'DE/monthly/2020-11/'}, {'Prefix': 'DE/monthly/2020-12/'}, {'Prefix': 'DE/monthly/2021-01/'}, {'Prefix': 'DE/monthly/2021-02/'}, {'Prefix': 'DE/monthly/2021-03/'}, {'Prefix': 'DE/monthly/2021-04/'}, {'Prefix': 'DE/monthly/2021-05/'}, {'Prefix': 'DE/monthly/2021-06/'}, {'Prefix': 'DE/monthly/2021-07/'}, {'Prefix': 'DE/monthly/2021-08/'}, {'Prefix': 'DE/monthly/2021-09/'}, {'Prefix': 'DE/monthly/2021-10/'}, {'Prefix': 'DE/monthly/2021-11/'}, {'Prefix': 'DE/monthly/2021-12/'}, {'Prefix': 'DE/monthly/2022-01/'}, {'Prefix': 'DE/monthly/2022-02/'}, {'Prefix': 'DE/monthly/2022-03/'}, {'Prefix': 'DE/monthly/2022-04/'}, {'Prefix': 'DE/monthly/2022-05/'}, {'Prefix': 'DE/monthly/2022-06/'}, {'Prefix': 'DE/monthly/2022-07/'}, {'Prefix': 'DE/monthly/2022-08/'}, {'Prefix': 'DE/monthly/2022-09/'}, {'Prefix': 'DE/monthly/2022-10/'}, {'Prefix': 'DE/monthly/2022-11/'}, {'Prefix': 'DE/monthly/2022-12/'}, {'Prefix': 'DE/monthly/2023-01/'}, {'Prefix': 'DE/monthly/2023-02/'}, {'Prefix': 'DE/monthly/2023-03/'}, {'Prefix': 'DE/monthly/2023-04/'}, {'Prefix': 'DE/monthly/2023-05/'}, {'Prefix': 'DE/monthly/2023-06/'}], 'EncodingType': 'url', 'KeyCount': 37}\n"
     ]
    }
   ],
   "source": [
    "# Get the list of objects in the S3 bucket\n",
    "response = s3.list_objects_v2(Bucket=aws_bucket_name, Prefix=prefix, Delimiter='/')\n",
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-07T01:51:56.850116892Z",
     "start_time": "2023-07-07T01:51:55.068651550Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading the files from the bucket"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file jobs.0.jsonl.gz...\n",
      "DE/monthly/2023-06/jobs.0.jsonl.gz\n",
      "/home/vboxuser/Documents/PycharmProjects/Bigdata-Processing-pipeline/data/raw/jobfeed-data-feeds/DE/monthly/2023-06/jobs.0.jsonl.gz\n",
      "/home/vboxuser/Documents/PycharmProjects/Bigdata-Processing-pipeline/data/raw/jobfeed-data-feeds/DE/monthly/2023-06/jobs.0.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Number of Months to download\n",
    "months = 1\n",
    "# Number of files per month to download\n",
    "files_per_month = 1\n",
    "# current project directory parent path\n",
    "ROOT_DIR = os.path.abspath(os.pardir)\n",
    "\n",
    "\n",
    "# Get the list of subfolders in the S3 bucket\n",
    "subfolders = [obj['Prefix'] for obj in response['CommonPrefixes']]\n",
    "# Get the last N subfolders - N = months of data to download\n",
    "subfolders = subfolders[-months:]\n",
    "\n",
    "\n",
    "filesToLoadInDF = []\n",
    "# Download files from each subfolder\n",
    "for subfolder in subfolders:\n",
    "    # Get the list of files in the subfolder\n",
    "    response = s3.list_objects_v2(Bucket=aws_bucket_name, Prefix=subfolder)\n",
    "    # Get the file paths\n",
    "    files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.jsonl.gz')]\n",
    "    # Only get the first N files\n",
    "    files = files[:files_per_month]\n",
    "\n",
    "    # filesToLoadInDF = [filesToLoadInDF.append(f) for f in files]\n",
    "\n",
    "    # Create the folder in your local machine\n",
    "    folder = ROOT_DIR + \"/data/raw/\" + aws_bucket_name + \"/\" + subfolder\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    # Download and extract each file\n",
    "    for file in files:\n",
    "        filename = file.rsplit(\"/\", 1)[1]\n",
    "        print('Downloading file {}...'.format(filename))\n",
    "        print(subfolder + filename)\n",
    "        print(folder + filename)\n",
    "\n",
    "        # Download and Save the file\n",
    "        s3.download_file(Filename=folder + filename, Bucket=aws_bucket_name, Key=subfolder + filename)\n",
    "\n",
    "        locaFilePath = os.path.join(folder + filename)\n",
    "        localExtractedFilePath = os.path.join(folder + filename[:-3])\n",
    "        print(localExtractedFilePath)\n",
    "        filesToLoadInDF.append(localExtractedFilePath)\n",
    "        # Extract the data from the gzipped file\n",
    "        with gzip.open(locaFilePath, 'rb') as gz_file, open(localExtractedFilePath, 'wb') as extract_file:\n",
    "            extract_file.write(gz_file.read())\n",
    "\n",
    "        # Delete the gzipped file\n",
    "        # os.remove(locaFilePath)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-07-07T01:51:56.820284894Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the Spark Session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"DE-Project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(filesToLoadInDF)\n",
    "df = spark.read.json(filesToLoadInDF)  # Use the extracted file paths here\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Postgres Connection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<connection object at 0x7f6120b802c0; dsn: 'user=airflow password=xxx dbname=airflow host=localhost port=5432', closed: 0>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Postgres Connection\n",
    "import psycopg2\n",
    "pgconn = psycopg2.connect(\n",
    "    host=os.getenv('POSTGRES_HOST'),\n",
    "    database=os.getenv('POSTGRES_DB'),\n",
    "    user=os.getenv('POSTGRES_USER'),\n",
    "    password=os.getenv('POSTGRES_PASSWORD'),\n",
    "    port=os.getenv('POSTGRES_PORT')\n",
    ")\n",
    "pgconn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-08T01:38:06.391807115Z",
     "start_time": "2023-07-08T01:38:06.173480650Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
